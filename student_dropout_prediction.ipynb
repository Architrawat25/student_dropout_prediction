{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqPx0H/HD0ltd5mIQFr70x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Architrawat25/student_dropout_prediction/blob/main/student_dropout_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKvgYtlZpFzL",
        "outputId": "f3286ae9-b824-439d-a6b9-4bd17b531b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.1/97.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘   STUDENT DROPOUT PREDICTION â€” COMPLETE EXPERIMENTAL PIPELINE                â•‘\n",
        "# â•‘                                                                              â•‘\n",
        "# â•‘   Runs on: Google Colab (CPU or GPU)                                         â•‘\n",
        "# â•‘   Produces: All tables, figures, and statistical test results                â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 1 â€” INSTALL DEPENDENCIES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Run this cell first. It installs all required libraries.\n",
        "# Colab already has scikit-learn, numpy, pandas, matplotlib, seaborn pre-installed.\n",
        "\n",
        "!pip install catboost optuna shap xgboost --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 2 â€” IMPORTS AND GLOBAL CONFIGURATION\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # non-interactive backend safe for Colab\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "# â”€â”€ Scikit-learn â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, roc_curve,\n",
        "    confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# â”€â”€ Boosting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "# â”€â”€ Hyperparameter Optimisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import optuna\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)  # suppress verbose output\n",
        "\n",
        "# â”€â”€ SHAP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import shap\n",
        "\n",
        "# â”€â”€ Statistical Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from scipy.stats import wilcoxon\n",
        "from scipy import stats\n",
        "\n",
        "# â”€â”€ Suppress non-critical warnings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# â”€â”€ Reproducibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# â”€â”€ Output directory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "OUTPUT_DIR = \"paper_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# â”€â”€ Optuna trials per fold â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Increase to 100 for thorough tuning (~30 min total on CPU)\n",
        "# 50 is a good balance (~15 min total on CPU)\n",
        "N_OPTUNA_TRIALS = 50\n",
        "\n",
        "print(\"âœ… All imports successful.\")\n",
        "print(f\"ğŸ“ Outputs will be saved to: ./{OUTPUT_DIR}/\")\n",
        "print(f\"ğŸ”§ Optuna trials per fold: {N_OPTUNA_TRIALS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4OcgggSqtVx",
        "outputId": "0f1637fb-c19a-4add-cd87-ebb57bfac8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All imports successful.\n",
            "ğŸ“ Outputs will be saved to: ./paper_outputs/\n",
            "ğŸ”§ Optuna trials per fold: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 3 â€” LOAD AND INSPECT DATASET\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# â”€â”€ INSTRUCTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Upload your CSV file to Colab by running the cell below.\n",
        "# It will open a file picker. Select your UCI dataset CSV file.\n",
        "# The code auto-detects the delimiter (comma or semicolon).\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"ğŸ“‚ Please upload your UCI dataset CSV file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename\n",
        "csv_filename = list(uploaded.keys())[0]\n",
        "print(f\"âœ… Uploaded: {csv_filename}\")\n",
        "\n",
        "# Auto-detect delimiter\n",
        "with open(csv_filename, 'r') as f:\n",
        "    first_line = f.readline()\n",
        "delimiter = ';' if first_line.count(';') > first_line.count(',') else ','\n",
        "print(f\"ğŸ” Detected delimiter: '{delimiter}'\")\n",
        "\n",
        "df_raw = pd.read_csv(csv_filename, delimiter=delimiter)\n",
        "print(f\"\\nğŸ“Š Dataset shape: {df_raw.shape}\")\n",
        "print(f\"ğŸ“‹ Columns: {list(df_raw.columns)}\")\n",
        "print(f\"\\nğŸ¯ Target variable distribution:\")\n",
        "print(df_raw['Target'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "7rHAksE1qtYF",
        "outputId": "6616057e-d084-4e5d-cb5b-a501afd679a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Please upload your UCI dataset CSV file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8192240f-805f-4b7f-8ddc-dc0d9a739494\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8192240f-805f-4b7f-8ddc-dc0d9a739494\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.csv to data (3).csv\n",
            "âœ… Uploaded: data (3).csv\n",
            "ğŸ” Detected delimiter: ';'\n",
            "\n",
            "ğŸ“Š Dataset shape: (4424, 37)\n",
            "ğŸ“‹ Columns: ['Marital status', 'Application mode', 'Application order', 'Course', 'Daytime/evening attendance\\t', 'Previous qualification', 'Previous qualification (grade)', 'Nacionality', \"Mother's qualification\", \"Father's qualification\", \"Mother's occupation\", \"Father's occupation\", 'Admission grade', 'Displaced', 'Educational special needs', 'Debtor', 'Tuition fees up to date', 'Gender', 'Scholarship holder', 'Age at enrollment', 'International', 'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)', 'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)', 'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)', 'Unemployment rate', 'Inflation rate', 'GDP', 'Target']\n",
            "\n",
            "ğŸ¯ Target variable distribution:\n",
            "Target\n",
            "Graduate    2209\n",
            "Dropout     1421\n",
            "Enrolled     794\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 4 â€” DATA PREPARATION AND BINARIZATION\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# â”€â”€ Step 1: Binarize target â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Dropout = 1 (positive class, at-risk)\n",
        "# Graduate + Enrolled = 0 (negative class, non-at-risk)\n",
        "df = df_raw.copy()\n",
        "df['Target_Binary'] = df['Target'].apply(lambda x: 1 if str(x).strip() == 'Dropout' else 0)\n",
        "\n",
        "print(\"ğŸ¯ Binarized target distribution:\")\n",
        "vc = df['Target_Binary'].value_counts()\n",
        "print(f\"  Non-Dropout (0): {vc[0]} ({vc[0]/len(df)*100:.1f}%)\")\n",
        "print(f\"  Dropout     (1): {vc[1]} ({vc[1]/len(df)*100:.1f}%)\")\n",
        "print(f\"  Class imbalance ratio: {vc[0]/vc[1]:.3f}\")\n",
        "\n",
        "# â”€â”€ Step 2: Define feature sets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Identify all feature columns (everything except original target and binary target)\n",
        "all_feature_cols = [c for c in df.columns if c not in ['Target', 'Target_Binary']]\n",
        "\n",
        "# â”€â”€ Macroeconomic feature names (exact UCI dataset column names) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# These are the three columns we ablate in the ablation study\n",
        "MACRO_COLS = [\n",
        "    'Unemployment rate',\n",
        "    'Inflation rate',\n",
        "    'GDP'\n",
        "]\n",
        "\n",
        "# Verify macroeconomic columns exist\n",
        "found_macro = [c for c in MACRO_COLS if c in all_feature_cols]\n",
        "missing_macro = [c for c in MACRO_COLS if c not in all_feature_cols]\n",
        "\n",
        "if missing_macro:\n",
        "    print(f\"\\nâš ï¸  Macroeconomic columns not found with exact names: {missing_macro}\")\n",
        "    print(\"   Available columns containing 'rate', 'gdp', 'inflation', 'unemployment':\")\n",
        "    for c in all_feature_cols:\n",
        "        if any(kw in c.lower() for kw in ['rate', 'gdp', 'inflation', 'unemploy']):\n",
        "            print(f\"   â†’ '{c}'\")\n",
        "    print(\"\\n   âš ï¸  Please update MACRO_COLS above to match your dataset's exact column names.\")\n",
        "else:\n",
        "    print(f\"\\nâœ… Macroeconomic columns identified: {found_macro}\")\n",
        "\n",
        "# â”€â”€ Step 3: Identify categorical columns for CatBoost â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# In the UCI dropout dataset, these columns are encoded as integers\n",
        "# but represent nominal categories\n",
        "CATEGORICAL_COLS_CATBOOST = [\n",
        "    'Marital status', 'Application mode', 'Application order', 'Course',\n",
        "    'Daytime/evening attendance', 'Previous qualification',\n",
        "    'Nacionality', \"Mother's qualification\", \"Father's qualification\",\n",
        "    \"Mother's occupation\", \"Father's occupation\", 'Displaced',\n",
        "    'Educational special needs', 'Debtor', 'Tuition fees up to date',\n",
        "    'Gender', 'Scholarship holder', 'International'\n",
        "]\n",
        "\n",
        "# Keep only those that actually exist in the dataset\n",
        "cat_cols_valid = [c for c in CATEGORICAL_COLS_CATBOOST if c in all_feature_cols]\n",
        "cat_col_indices = [all_feature_cols.index(c) for c in cat_cols_valid]\n",
        "\n",
        "print(f\"\\nğŸ“¦ Total features: {len(all_feature_cols)}\")\n",
        "print(f\"ğŸ“¦ Features in full model (Model A): {len(all_feature_cols)}\")\n",
        "print(f\"ğŸ“¦ Features in ablated model (Model B): {len(all_feature_cols) - len(found_macro)}\")\n",
        "\n",
        "# â”€â”€ Step 4: Prepare X, y â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "X_full = df[all_feature_cols].values.astype(float)\n",
        "X_ablated = df[[c for c in all_feature_cols if c not in found_macro]].values.astype(float)\n",
        "y = df['Target_Binary'].values\n",
        "\n",
        "feature_names_full = all_feature_cols\n",
        "feature_names_ablated = [c for c in all_feature_cols if c not in found_macro]\n",
        "\n",
        "# â”€â”€ Step 5: Class weight ratio for boosting models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "n_neg = np.sum(y == 0)\n",
        "n_pos = np.sum(y == 1)\n",
        "class_weight_ratio = n_neg / n_pos\n",
        "print(f\"\\nâš–ï¸  Class weight ratio (neg/pos) for boosting: {class_weight_ratio:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I6so-uqqtas",
        "outputId": "50c1b302-56cb-4a7a-92f2-c5776fc02987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ Binarized target distribution:\n",
            "  Non-Dropout (0): 3003 (67.9%)\n",
            "  Dropout     (1): 1421 (32.1%)\n",
            "  Class imbalance ratio: 2.113\n",
            "\n",
            "âœ… Macroeconomic columns identified: ['Unemployment rate', 'Inflation rate', 'GDP']\n",
            "\n",
            "ğŸ“¦ Total features: 36\n",
            "ğŸ“¦ Features in full model (Model A): 36\n",
            "ğŸ“¦ Features in ablated model (Model B): 33\n",
            "\n",
            "âš–ï¸  Class weight ratio (neg/pos) for boosting: 2.1133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 5 â€” HELPER FUNCTIONS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def compute_metrics(y_true, y_pred, y_prob):\n",
        "    \"\"\"Compute all 5 evaluation metrics.\"\"\"\n",
        "    return {\n",
        "        'accuracy':  accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'recall':    recall_score(y_true, y_pred, zero_division=0),\n",
        "        'f1':        f1_score(y_true, y_pred, zero_division=0),\n",
        "        'auc_roc':   roc_auc_score(y_true, y_prob)\n",
        "    }\n",
        "\n",
        "\n",
        "def summarize_cv_results(fold_metrics):\n",
        "    \"\"\"Compute mean Â± std across folds for all metrics.\"\"\"\n",
        "    summary = {}\n",
        "    metrics = fold_metrics[0].keys()\n",
        "    for m in metrics:\n",
        "        vals = [fold[m] for fold in fold_metrics]\n",
        "        summary[m] = {'mean': np.mean(vals), 'std': np.std(vals), 'values': vals}\n",
        "    return summary\n",
        "\n",
        "\n",
        "def print_summary_table(name, summary):\n",
        "    \"\"\"Print a nicely formatted results table.\"\"\"\n",
        "    print(f\"\\n{'â”€'*65}\")\n",
        "    print(f\"  {name}\")\n",
        "    print(f\"{'â”€'*65}\")\n",
        "    print(f\"  {'Metric':<15} {'Mean':>10} {'Std Dev':>10} {'Values (per fold)'}\")\n",
        "    print(f\"{'â”€'*65}\")\n",
        "    for m, v in summary.items():\n",
        "        vals_str = \"  \".join([f\"{x:.4f}\" for x in v['values']])\n",
        "        print(f\"  {m:<15} {v['mean']:>10.4f} {v['std']:>10.4f}   [{vals_str}]\")\n",
        "    print(f\"{'â”€'*65}\")\n",
        "\n",
        "\n",
        "def apply_mi_feature_selection(X_train, y_train, X_val, threshold_percentile=5):\n",
        "    \"\"\"\n",
        "    Mutual information feature selection on training fold only.\n",
        "    Removes features in the bottom `threshold_percentile` of MI scores.\n",
        "    Returns selected X_train, X_val and selected feature indices.\n",
        "    \"\"\"\n",
        "    mi_scores = mutual_info_classif(X_train, y_train, random_state=RANDOM_SEED)\n",
        "    threshold = np.percentile(mi_scores, threshold_percentile)\n",
        "    selected_idx = np.where(mi_scores >= threshold)[0]\n",
        "    return X_train[:, selected_idx], X_val[:, selected_idx], selected_idx, mi_scores\n",
        "\n",
        "\n",
        "# â”€â”€ DeLong's Test Implementation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def delong_roc_test(y_true, y_score1, y_score2):\n",
        "    \"\"\"\n",
        "    DeLong's test to compare two correlated AUC-ROC values.\n",
        "    Returns: z-statistic, p-value, AUC1, AUC2.\n",
        "    Based on: DeLong et al. (1988) Biometrics 44(3):837-845.\n",
        "    \"\"\"\n",
        "    def compute_midrank(x):\n",
        "        J = np.argsort(x)\n",
        "        Z = x[J]\n",
        "        N = len(x)\n",
        "        T = np.zeros(N, dtype=float)\n",
        "        i = 0\n",
        "        while i < N:\n",
        "            j = i\n",
        "            while j < N and Z[j] == Z[i]:\n",
        "                j += 1\n",
        "            T[i:j] = 0.5 * (i + j - 1)\n",
        "            i = j\n",
        "        T2 = np.empty(N, dtype=float)\n",
        "        T2[J] = T + 1\n",
        "        return T2\n",
        "\n",
        "    def fastDeLong(predictions_sorted_transposed, label_1_count):\n",
        "        m = label_1_count\n",
        "        n = predictions_sorted_transposed.shape[1] - m\n",
        "        positive_examples = predictions_sorted_transposed[:, :m]\n",
        "        negative_examples = predictions_sorted_transposed[:, m:]\n",
        "        k = predictions_sorted_transposed.shape[0]\n",
        "\n",
        "        tx = np.empty([k, m], dtype=float)\n",
        "        ty = np.empty([k, n], dtype=float)\n",
        "        tz = np.empty([k, m + n], dtype=float)\n",
        "\n",
        "        for r in range(k):\n",
        "            tx[r, :] = compute_midrank(positive_examples[r, :])\n",
        "            ty[r, :] = compute_midrank(negative_examples[r, :])\n",
        "            tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
        "\n",
        "        aucs = (tz[:, :m].sum(axis=1) - tx.sum(axis=1)) / (m * n)\n",
        "        v01 = (tz[:, :m] - tx[:, :]) / n\n",
        "        v10 = 1. - (tz[:, m:] - ty[:, :]) / m\n",
        "        sx = np.cov(v01)\n",
        "        sy = np.cov(v10)\n",
        "        delongcov = sx / m + sy / n\n",
        "        return aucs, delongcov\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_score1 = np.asarray(y_score1)\n",
        "    y_score2 = np.asarray(y_score2)\n",
        "\n",
        "    sorted_indices = np.argsort(y_true)[::-1]\n",
        "    y_true_sorted = y_true[sorted_indices]\n",
        "    label_1_count = int(y_true_sorted.sum())\n",
        "\n",
        "    predictions_sorted_transposed = np.vstack([\n",
        "        y_score1[sorted_indices],\n",
        "        y_score2[sorted_indices]\n",
        "    ])\n",
        "\n",
        "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
        "    auc1, auc2 = aucs[0], aucs[1]\n",
        "\n",
        "    # Variance of (AUC1 - AUC2)\n",
        "    var = delongcov[0, 0] + delongcov[1, 1] - 2 * delongcov[0, 1]\n",
        "    if var <= 0:\n",
        "        z_stat = 0.0\n",
        "        p_value = 1.0\n",
        "    else:\n",
        "        z_stat = (auc1 - auc2) / np.sqrt(var)\n",
        "        p_value = 2 * stats.norm.sf(abs(z_stat))\n",
        "\n",
        "    return z_stat, p_value, auc1, auc2\n",
        "\n",
        "\n",
        "print(\"âœ… Helper functions defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT72KpX9qtdT",
        "outputId": "05238b02-fea0-4c2c-e980-2a43d53e98c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Helper functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 6 â€” LOGISTIC REGRESSION WITH STRATIFIED 5-FOLD CV\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"â•\"*65)\n",
        "print(\"  MODEL 1: LOGISTIC REGRESSION\")\n",
        "print(\"â•\"*65)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "lr_fold_metrics = []\n",
        "lr_best_params_per_fold = []\n",
        "\n",
        "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_full, y)):\n",
        "    print(f\"\\n  Fold {fold_idx+1}/5 ...\", end=\" \")\n",
        "\n",
        "    X_train_fold, X_val_fold = X_full[train_idx], X_full[val_idx]\n",
        "    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "    # â”€â”€ Mutual Information Feature Selection (on training fold only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    X_train_sel, X_val_sel, sel_idx, mi_scores = apply_mi_feature_selection(\n",
        "        X_train_fold, y_train_fold, X_val_fold\n",
        "    )\n",
        "\n",
        "    # â”€â”€ Standard scaling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_sel)\n",
        "    X_val_scaled   = scaler.transform(X_val_sel)\n",
        "\n",
        "    # â”€â”€ Grid Search for C â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    C_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "    lr_gs = GridSearchCV(\n",
        "        LogisticRegression(\n",
        "            class_weight='balanced',\n",
        "            max_iter=2000,\n",
        "            solver='lbfgs',\n",
        "            random_state=RANDOM_SEED\n",
        "        ),\n",
        "        C_grid,\n",
        "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED),\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    lr_gs.fit(X_train_scaled, y_train_fold)\n",
        "    best_lr = lr_gs.best_estimator_\n",
        "    lr_best_params_per_fold.append(lr_gs.best_params_)\n",
        "\n",
        "    # â”€â”€ Predict â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    y_pred  = best_lr.predict(X_val_scaled)\n",
        "    y_prob  = best_lr.predict_proba(X_val_scaled)[:, 1]\n",
        "\n",
        "    metrics = compute_metrics(y_val_fold, y_pred, y_prob)\n",
        "    lr_fold_metrics.append(metrics)\n",
        "    print(f\"AUC={metrics['auc_roc']:.4f}  F1={metrics['f1']:.4f}  Best C={lr_gs.best_params_['C']}\")\n",
        "\n",
        "lr_summary = summarize_cv_results(lr_fold_metrics)\n",
        "print_summary_table(\"LOGISTIC REGRESSION â€” Cross-Validation Summary\", lr_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPJLkl_aqtfo",
        "outputId": "ff1d44d7-a640-416b-89b7-a6b61ce35736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "  MODEL 1: LOGISTIC REGRESSION\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "  Fold 1/5 ... AUC=0.9102  F1=0.8062  Best C=0.1\n",
            "\n",
            "  Fold 2/5 ... AUC=0.9114  F1=0.7939  Best C=10\n",
            "\n",
            "  Fold 3/5 ... AUC=0.9255  F1=0.7945  Best C=0.1\n",
            "\n",
            "  Fold 4/5 ... AUC=0.9248  F1=0.7866  Best C=1\n",
            "\n",
            "  Fold 5/5 ... AUC=0.9192  F1=0.8049  Best C=0.1\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  LOGISTIC REGRESSION â€” Cross-Validation Summary\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  Metric                Mean    Std Dev Values (per fold)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  accuracy            0.8660     0.0062   [0.8723  0.8633  0.8644  0.8565  0.8733]\n",
            "  precision           0.7760     0.0155   [0.7886  0.7690  0.7733  0.7524  0.7966]\n",
            "  recall              0.8198     0.0042   [0.8246  0.8204  0.8169  0.8239  0.8134]\n",
            "  f1                  0.7972     0.0074   [0.8062  0.7939  0.7945  0.7866  0.8049]\n",
            "  auc_roc             0.9182     0.0064   [0.9102  0.9114  0.9255  0.9248  0.9192]\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 7 â€” XGBOOST WITH OPTUNA TUNING\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"â•\"*65)\n",
        "print(\"  MODEL 2: XGBOOST\")\n",
        "print(\"â•\"*65)\n",
        "\n",
        "xgb_fold_metrics = []\n",
        "xgb_best_params_per_fold = []\n",
        "\n",
        "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_full, y)):\n",
        "    print(f\"\\n  Fold {fold_idx+1}/5 â€” Optuna tuning ({N_OPTUNA_TRIALS} trials) ...\", end=\" \")\n",
        "\n",
        "    X_train_fold, X_val_fold = X_full[train_idx], X_full[val_idx]\n",
        "    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "    # â”€â”€ MI Feature Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    X_train_sel, X_val_sel, sel_idx, _ = apply_mi_feature_selection(\n",
        "        X_train_fold, y_train_fold, X_val_fold\n",
        "    )\n",
        "\n",
        "    # â”€â”€ Inner split for Optuna â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    inner_skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "    inner_splits = list(inner_skf.split(X_train_sel, y_train_fold))\n",
        "\n",
        "    def xgb_objective(trial):\n",
        "        params = {\n",
        "            'n_estimators':      trial.suggest_int('n_estimators', 100, 1000),\n",
        "            'max_depth':         trial.suggest_int('max_depth', 3, 10),\n",
        "            'learning_rate':     trial.suggest_float('learning_rate', 0.01, 0.30, log=True),\n",
        "            'subsample':         trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree':  trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "            'reg_lambda':        trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
        "            'reg_alpha':         trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
        "            'scale_pos_weight':  class_weight_ratio,\n",
        "            'use_label_encoder': False,\n",
        "            'eval_metric':       'auc',\n",
        "            'random_state':      RANDOM_SEED,\n",
        "            'n_jobs':            -1\n",
        "        }\n",
        "        # Inner 5-fold CV for Optuna\n",
        "        inner_aucs = []\n",
        "        for tr_idx, vl_idx in inner_splits:\n",
        "            Xtr, Xvl = X_train_sel[tr_idx], X_train_sel[vl_idx]\n",
        "            ytr, yvl = y_train_fold[tr_idx], y_train_fold[vl_idx]\n",
        "            model = xgb.XGBClassifier(**params)\n",
        "            model.fit(Xtr, ytr, verbose=False)\n",
        "            prob = model.predict_proba(Xvl)[:, 1]\n",
        "            inner_aucs.append(roc_auc_score(yvl, prob))\n",
        "        return np.mean(inner_aucs)\n",
        "\n",
        "    study_xgb = optuna.create_study(direction='maximize',\n",
        "                                     sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
        "    study_xgb.optimize(xgb_objective, n_trials=N_OPTUNA_TRIALS, show_progress_bar=False)\n",
        "\n",
        "    best_p = study_xgb.best_params\n",
        "    xgb_best_params_per_fold.append(best_p)\n",
        "\n",
        "    # â”€â”€ Train final model on full training fold â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    best_xgb = xgb.XGBClassifier(\n",
        "        **best_p,\n",
        "        scale_pos_weight=class_weight_ratio,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='auc',\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    best_xgb.fit(X_train_sel, y_train_fold)\n",
        "\n",
        "    y_pred = best_xgb.predict(X_val_sel)\n",
        "    y_prob = best_xgb.predict_proba(X_val_sel)[:, 1]\n",
        "\n",
        "    metrics = compute_metrics(y_val_fold, y_pred, y_prob)\n",
        "    xgb_fold_metrics.append(metrics)\n",
        "    print(f\"AUC={metrics['auc_roc']:.4f}  F1={metrics['f1']:.4f}  Best LR={best_p.get('learning_rate', 'N/A'):.4f}\")\n",
        "\n",
        "xgb_summary = summarize_cv_results(xgb_fold_metrics)\n",
        "print_summary_table(\"XGBOOST â€” Cross-Validation Summary\", xgb_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ-DXGmPqtiQ",
        "outputId": "00e1ebad-e892-4595-af95-38c2146bbe4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "  MODEL 2: XGBOOST\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "  Fold 1/5 â€” Optuna tuning (50 trials) ... AUC=0.9187  F1=0.7909  Best LR=0.0183\n",
            "\n",
            "  Fold 2/5 â€” Optuna tuning (50 trials) ... AUC=0.9231  F1=0.7892  Best LR=0.0177\n",
            "\n",
            "  Fold 3/5 â€” Optuna tuning (50 trials) ... AUC=0.9321  F1=0.7944  Best LR=0.0518\n",
            "\n",
            "  Fold 4/5 â€” Optuna tuning (50 trials) ... AUC=0.9346  F1=0.8055  Best LR=0.0232\n",
            "\n",
            "  Fold 5/5 â€” Optuna tuning (50 trials) ... AUC=0.9302  F1=0.8198  Best LR=0.0256\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  XGBOOST â€” Cross-Validation Summary\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  Metric                Mean    Std Dev Values (per fold)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  accuracy            0.8712     0.0072   [0.8644  0.8678  0.8667  0.8723  0.8846]\n",
            "  precision           0.7981     0.0149   [0.7855  0.8081  0.7862  0.7879  0.8227]\n",
            "  recall              0.8023     0.0184   [0.7965  0.7711  0.8028  0.8239  0.8169]\n",
            "  f1                  0.8000     0.0114   [0.7909  0.7892  0.7944  0.8055  0.8198]\n",
            "  auc_roc             0.9278     0.0059   [0.9187  0.9231  0.9321  0.9346  0.9302]\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 8 â€” CATBOOST MODEL A (FULL FEATURES) WITH OPTUNA TUNING\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"â•\"*65)\n",
        "print(\"  MODEL 3: CATBOOST â€” MODEL A (Full Feature Set)\")\n",
        "print(\"â•\"*65)\n",
        "\n",
        "cat_A_fold_metrics = []\n",
        "cat_A_best_params_per_fold = []\n",
        "cat_A_val_probs  = {}   # store per-fold probs for DeLong test\n",
        "cat_A_val_labels = {}\n",
        "\n",
        "# Get categorical feature indices that remain after MI selection\n",
        "# We need to re-identify them after selection â€” handled per fold below\n",
        "\n",
        "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_full, y)):\n",
        "    print(f\"\\n  Fold {fold_idx+1}/5 â€” Optuna tuning ({N_OPTUNA_TRIALS} trials) ...\", end=\" \")\n",
        "\n",
        "    X_train_fold, X_val_fold = X_full[train_idx], X_full[val_idx]\n",
        "    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "    # â”€â”€ MI Feature Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    X_train_sel, X_val_sel, sel_idx, _ = apply_mi_feature_selection(\n",
        "        X_train_fold, y_train_fold, X_val_fold\n",
        "    )\n",
        "\n",
        "    # Re-map categorical column indices after feature selection\n",
        "    # selected_features = [feature_names_full[i] for i in sel_idx]\n",
        "    # cat_indices_fold = [selected_features.index(c) for c in cat_cols_valid\n",
        "    #                     if c in selected_features]\n",
        "\n",
        "    # â”€â”€ Inner split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    inner_skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "    inner_splits = list(inner_skf.split(X_train_sel, y_train_fold))\n",
        "\n",
        "    def cat_objective(trial):\n",
        "        params = {\n",
        "            'iterations':        trial.suggest_int('iterations', 100, 1000),\n",
        "            'depth':             trial.suggest_int('depth', 3, 10),\n",
        "            'learning_rate':     trial.suggest_float('learning_rate', 0.01, 0.30, log=True),\n",
        "            'l2_leaf_reg':       trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
        "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
        "            'random_strength':   trial.suggest_float('random_strength', 0.0, 1.0),\n",
        "        }\n",
        "        inner_aucs = []\n",
        "        for tr_idx, vl_idx in inner_splits:\n",
        "            Xtr, Xvl = X_train_sel[tr_idx], X_train_sel[vl_idx]\n",
        "            ytr, yvl = y_train_fold[tr_idx], y_train_fold[vl_idx]\n",
        "            w = np.where(ytr == 1, class_weight_ratio, 1.0)\n",
        "            model = CatBoostClassifier(\n",
        "                **params,\n",
        "                class_weights={0: 1.0, 1: class_weight_ratio},\n",
        "                random_seed=RANDOM_SEED,\n",
        "                verbose=0,\n",
        "                allow_writing_files=False\n",
        "            )\n",
        "            model.fit(Xtr, ytr)\n",
        "            prob = model.predict_proba(Xvl)[:, 1]\n",
        "            inner_aucs.append(roc_auc_score(yvl, prob))\n",
        "        return np.mean(inner_aucs)\n",
        "\n",
        "    study_cat_A = optuna.create_study(direction='maximize',\n",
        "                                       sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
        "    study_cat_A.optimize(cat_objective, n_trials=N_OPTUNA_TRIALS, show_progress_bar=False)\n",
        "\n",
        "    best_p = study_cat_A.best_params\n",
        "    cat_A_best_params_per_fold.append(best_p)\n",
        "\n",
        "    # â”€â”€ Final model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    best_cat_A = CatBoostClassifier(\n",
        "        **best_p,\n",
        "        class_weights={0: 1.0, 1: class_weight_ratio},\n",
        "        random_seed=RANDOM_SEED,\n",
        "        verbose=0,\n",
        "        allow_writing_files=False\n",
        "    )\n",
        "    best_cat_A.fit(X_train_sel, y_train_fold)\n",
        "\n",
        "    y_pred = best_cat_A.predict(X_val_sel)\n",
        "    y_prob = best_cat_A.predict_proba(X_val_sel)[:, 1]\n",
        "\n",
        "    metrics = compute_metrics(y_val_fold, y_pred, y_prob)\n",
        "    cat_A_fold_metrics.append(metrics)\n",
        "    cat_A_val_probs[fold_idx]  = y_prob\n",
        "    cat_A_val_labels[fold_idx] = y_val_fold\n",
        "\n",
        "    print(f\"AUC={metrics['auc_roc']:.4f}  F1={metrics['f1']:.4f}  Depth={best_p.get('depth')}\")\n",
        "\n",
        "cat_A_summary = summarize_cv_results(cat_A_fold_metrics)\n",
        "print_summary_table(\"CATBOOST (Model A â€” Full Features) â€” Cross-Validation Summary\", cat_A_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrKlFtiZr8CN",
        "outputId": "cc7603e0-a926-4b68-9ea5-c91ade1e440b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "  MODEL 3: CATBOOST â€” MODEL A (Full Feature Set)\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "  Fold 1/5 â€” Optuna tuning (50 trials) ... AUC=0.9134  F1=0.7896  Depth=6\n",
            "\n",
            "  Fold 2/5 â€” Optuna tuning (50 trials) ... AUC=0.9254  F1=0.8063  Depth=6\n",
            "\n",
            "  Fold 3/5 â€” Optuna tuning (50 trials) ... AUC=0.9360  F1=0.7958  Depth=5\n",
            "\n",
            "  Fold 4/5 â€” Optuna tuning (50 trials) ... AUC=0.9306  F1=0.7951  Depth=7\n",
            "\n",
            "  Fold 5/5 â€” Optuna tuning (50 trials) ... AUC=0.9308  F1=0.8211  Depth=5\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  CATBOOST (Model A â€” Full Features) â€” Cross-Validation Summary\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  Metric                Mean    Std Dev Values (per fold)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  accuracy            0.8716     0.0074   [0.8633  0.8746  0.8678  0.8678  0.8846]\n",
            "  precision           0.7960     0.0123   [0.7828  0.7993  0.7889  0.7909  0.8182]\n",
            "  recall              0.8072     0.0101   [0.7965  0.8134  0.8028  0.7993  0.8239]\n",
            "  f1                  0.8016     0.0111   [0.7896  0.8063  0.7958  0.7951  0.8211]\n",
            "  auc_roc             0.9273     0.0077   [0.9134  0.9254  0.9360  0.9306  0.9308]\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 9 â€” CATBOOST MODEL B (ABLATED â€” NO MACROECONOMIC FEATURES)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"â•\"*65)\n",
        "print(\"  MODEL 4: CATBOOST â€” MODEL B (Ablated: No Macroeconomic Features)\")\n",
        "print(\"â•\"*65)\n",
        "\n",
        "cat_B_fold_metrics = []\n",
        "cat_B_val_probs  = {}\n",
        "cat_B_val_labels = {}\n",
        "\n",
        "# Categorical column indices for the ablated feature set\n",
        "cat_cols_ablated_valid = [c for c in cat_cols_valid if c not in found_macro]\n",
        "\n",
        "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_ablated, y)):\n",
        "    print(f\"\\n  Fold {fold_idx+1}/5 â€” Optuna tuning ({N_OPTUNA_TRIALS} trials) ...\", end=\" \")\n",
        "\n",
        "    X_train_fold, X_val_fold = X_ablated[train_idx], X_ablated[val_idx]\n",
        "    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "    # â”€â”€ MI Feature Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    X_train_sel, X_val_sel, sel_idx, _ = apply_mi_feature_selection(\n",
        "        X_train_fold, y_train_fold, X_val_fold\n",
        "    )\n",
        "\n",
        "    # selected_features = [feature_names_ablated[i] for i in sel_idx]\n",
        "    # cat_indices_fold = [selected_features.index(c) for c in cat_cols_ablated_valid\n",
        "    #                     if c in selected_features]\n",
        "\n",
        "    # â”€â”€ Inner split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    inner_skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "    inner_splits = list(inner_skf.split(X_train_sel, y_train_fold))\n",
        "\n",
        "    def cat_B_objective(trial):\n",
        "        params = {\n",
        "            'iterations':          trial.suggest_int('iterations', 100, 1000),\n",
        "            'depth':               trial.suggest_int('depth', 3, 10),\n",
        "            'learning_rate':       trial.suggest_float('learning_rate', 0.01, 0.30, log=True),\n",
        "            'l2_leaf_reg':         trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
        "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
        "            'random_strength':     trial.suggest_float('random_strength', 0.0, 1.0),\n",
        "        }\n",
        "        inner_aucs = []\n",
        "        for tr_idx, vl_idx in inner_splits:\n",
        "            Xtr, Xvl = X_train_sel[tr_idx], X_train_sel[vl_idx]\n",
        "            ytr, yvl = y_train_fold[tr_idx], y_train_fold[vl_idx]\n",
        "            model = CatBoostClassifier(\n",
        "                **params,\n",
        "                class_weights={0: 1.0, 1: class_weight_ratio},\n",
        "                random_seed=RANDOM_SEED,\n",
        "                verbose=0,\n",
        "                allow_writing_files=False\n",
        "            )\n",
        "            model.fit(Xtr, ytr)\n",
        "            prob = model.predict_proba(Xvl)[:, 1]\n",
        "            inner_aucs.append(roc_auc_score(yvl, prob))\n",
        "        return np.mean(inner_aucs)\n",
        "\n",
        "    study_cat_B = optuna.create_study(direction='maximize',\n",
        "                                       sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
        "    study_cat_B.optimize(cat_B_objective, n_trials=N_OPTUNA_TRIALS, show_progress_bar=False)\n",
        "\n",
        "    best_p = study_cat_B.best_params\n",
        "\n",
        "    best_cat_B = CatBoostClassifier(\n",
        "        **best_p,\n",
        "        class_weights={0: 1.0, 1: class_weight_ratio},\n",
        "        random_seed=RANDOM_SEED,\n",
        "        verbose=0,\n",
        "        allow_writing_files=False\n",
        "    )\n",
        "    best_cat_B.fit(X_train_sel, y_train_fold)\n",
        "\n",
        "    y_pred = best_cat_B.predict(X_val_sel)\n",
        "    y_prob = best_cat_B.predict_proba(X_val_sel)[:, 1]\n",
        "\n",
        "    metrics = compute_metrics(y_val_fold, y_pred, y_prob)\n",
        "    cat_B_fold_metrics.append(metrics)\n",
        "    cat_B_val_probs[fold_idx]  = y_prob\n",
        "    cat_B_val_labels[fold_idx] = y_val_fold\n",
        "\n",
        "    print(f\"AUC={metrics['auc_roc']:.4f}  F1={metrics['f1']:.4f}  Depth={best_p.get('depth')}\")\n",
        "\n",
        "cat_B_summary = summarize_cv_results(cat_B_fold_metrics)\n",
        "print_summary_table(\"CATBOOST (Model B â€” Ablated) â€” Cross-Validation Summary\", cat_B_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr2iLqRcr8Id",
        "outputId": "9bb7dea4-2eab-4e23-917f-fde6fcbaa6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "  MODEL 4: CATBOOST â€” MODEL B (Ablated: No Macroeconomic Features)\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "  Fold 1/5 â€” Optuna tuning (50 trials) ... AUC=0.9117  F1=0.7774  Depth=8\n",
            "\n",
            "  Fold 2/5 â€” Optuna tuning (50 trials) ... AUC=0.9217  F1=0.8035  Depth=6\n",
            "\n",
            "  Fold 3/5 â€” Optuna tuning (50 trials) ... AUC=0.9342  F1=0.7965  Depth=6\n",
            "\n",
            "  Fold 4/5 â€” Optuna tuning (50 trials) ... AUC=0.9305  F1=0.7931  Depth=4\n",
            "\n",
            "  Fold 5/5 â€” Optuna tuning (50 trials) ... AUC=0.9264  F1=0.8092  Depth=5\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  CATBOOST (Model B â€” Ablated) â€” Cross-Validation Summary\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  Metric                Mean    Std Dev Values (per fold)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  accuracy            0.8680     0.0069   [0.8576  0.8723  0.8678  0.8644  0.8778]\n",
            "  precision           0.7906     0.0121   [0.7829  0.7938  0.7869  0.7770  0.8121]\n",
            "  recall              0.8016     0.0150   [0.7719  0.8134  0.8063  0.8099  0.8063]\n",
            "  f1                  0.7959     0.0108   [0.7774  0.8035  0.7965  0.7931  0.8092]\n",
            "  auc_roc             0.9249     0.0078   [0.9117  0.9217  0.9342  0.9305  0.9264]\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 10 â€” STATISTICAL SIGNIFICANCE TESTS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"â•\"*65)\n",
        "print(\"  STATISTICAL SIGNIFICANCE TESTS\")\n",
        "print(\"â•\"*65)\n",
        "\n",
        "# â”€â”€ AUC vectors per fold â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "lr_aucs   = [m['auc_roc'] for m in lr_fold_metrics]\n",
        "xgb_aucs  = [m['auc_roc'] for m in xgb_fold_metrics]\n",
        "cat_A_aucs = [m['auc_roc'] for m in cat_A_fold_metrics]\n",
        "cat_B_aucs = [m['auc_roc'] for m in cat_B_fold_metrics]\n",
        "\n",
        "print(f\"\\n  Per-fold AUC-ROC values:\")\n",
        "print(f\"  LR     : {[f'{v:.4f}' for v in lr_aucs]}\")\n",
        "print(f\"  XGBoost: {[f'{v:.4f}' for v in xgb_aucs]}\")\n",
        "print(f\"  Cat A  : {[f'{v:.4f}' for v in cat_A_aucs]}\")\n",
        "print(f\"  Cat B  : {[f'{v:.4f}' for v in cat_B_aucs]}\")\n",
        "\n",
        "# â”€â”€ Wilcoxon Signed-Rank Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# NOTE: With n=5, minimum achievable two-tailed p-value is ~0.0625\n",
        "# This is mathematically unavoidable and must be reported in the paper\n",
        "\n",
        "def safe_wilcoxon(a, b, label):\n",
        "    \"\"\"Run Wilcoxon test with full reporting including power constraint note.\"\"\"\n",
        "    diffs = np.array(a) - np.array(b)\n",
        "    n_pos = np.sum(diffs > 0)\n",
        "    n_neg = np.sum(diffs < 0)\n",
        "    n_tie = np.sum(diffs == 0)\n",
        "\n",
        "    if np.all(diffs == 0):\n",
        "        print(f\"\\n  {label}\")\n",
        "        print(f\"  All differences are zero. Cannot perform Wilcoxon test.\")\n",
        "        return None, None, diffs\n",
        "\n",
        "    try:\n",
        "        stat, p = wilcoxon(a, b, alternative='two-sided')\n",
        "    except ValueError as e:\n",
        "        print(f\"\\n  {label}: {e}\")\n",
        "        return None, None, diffs\n",
        "\n",
        "    direction = \"A > B\" if n_pos > n_neg else \"B > A\" if n_neg > n_pos else \"Mixed\"\n",
        "    print(f\"\\n  {label}\")\n",
        "    print(f\"  W = {stat:.1f}   p = {p:.4f}   Direction: {direction} \"\n",
        "          f\"(+:{n_pos}, -:{n_neg}, ties:{n_tie})\")\n",
        "    print(f\"  [Note: min. achievable p with n=5 is â‰ˆ0.0625]\")\n",
        "    if p < 0.05:\n",
        "        print(f\"  â†’ REJECT null at Î±=0.05 âœ…\")\n",
        "    elif p < 0.10:\n",
        "        print(f\"  â†’ Borderline: consistent direction, limited by n=5 power constraint\")\n",
        "    else:\n",
        "        print(f\"  â†’ FAIL TO REJECT null at Î±=0.05\")\n",
        "    return stat, p, diffs\n",
        "\n",
        "print(\"\\nâ”€â”€ Wilcoxon Signed-Rank Tests (Pairwise AUC-ROC, n=5 folds) â”€â”€\")\n",
        "\n",
        "w1, p1, _ = safe_wilcoxon(cat_A_aucs, lr_aucs,   \"CatBoost (Model A) vs. Logistic Regression\")\n",
        "w2, p2, _ = safe_wilcoxon(xgb_aucs,  lr_aucs,   \"XGBoost vs. Logistic Regression\")\n",
        "w3, p3, _ = safe_wilcoxon(cat_A_aucs, xgb_aucs,  \"CatBoost (Model A) vs. XGBoost\")\n",
        "\n",
        "# â”€â”€ DeLong's Test â€” Ablation Study (Model A vs. Model B) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\nâ”€â”€ DeLong's Test (Ablation: Model A vs. Model B) â”€â”€\")\n",
        "\n",
        "# Aggregate all out-of-fold predictions for DeLong test\n",
        "all_y_true_A, all_probs_A = [], []\n",
        "all_y_true_B, all_probs_B = [], []\n",
        "\n",
        "for fold_idx in range(5):\n",
        "    all_y_true_A.extend(cat_A_val_labels[fold_idx])\n",
        "    all_probs_A.extend(cat_A_val_probs[fold_idx])\n",
        "    all_y_true_B.extend(cat_B_val_labels[fold_idx])\n",
        "    all_probs_B.extend(cat_B_val_probs[fold_idx])\n",
        "\n",
        "all_y_true_A = np.array(all_y_true_A)\n",
        "all_probs_A  = np.array(all_probs_A)\n",
        "all_probs_B  = np.array(all_probs_B)\n",
        "\n",
        "z_delong, p_delong, auc_A_full, auc_B_full = delong_roc_test(\n",
        "    all_y_true_A, all_probs_A, all_probs_B\n",
        ")\n",
        "\n",
        "print(f\"\\n  Model A (Full) AUC (DeLong):  {auc_A_full:.4f}\")\n",
        "print(f\"  Model B (Ablated) AUC (DeLong): {auc_B_full:.4f}\")\n",
        "print(f\"  Î”AUC = {auc_B_full - auc_A_full:.4f}\")\n",
        "print(f\"  DeLong z-statistic = {z_delong:.4f}\")\n",
        "print(f\"  p-value = {p_delong:.4f}\")\n",
        "if p_delong < 0.05:\n",
        "    print(f\"  â†’ REJECT Hâ‚€â‚‚ at Î±=0.05: Macroeconomic exclusion significantly degrades performance âœ…\")\n",
        "else:\n",
        "    print(f\"  â†’ FAIL TO REJECT Hâ‚€â‚‚ at Î±=0.05: No statistically significant degradation\")\n",
        "    print(f\"     â‡’ Empirical validation of limited marginal predictive contribution of\")\n",
        "    print(f\"       macroeconomic indicators.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQLJevBJsKrd",
        "outputId": "b068a9b2-28d6-430d-bca3-5c6753695869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "  STATISTICAL SIGNIFICANCE TESTS\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "  Per-fold AUC-ROC values:\n",
            "  LR     : ['0.9102', '0.9114', '0.9255', '0.9248', '0.9192']\n",
            "  XGBoost: ['0.9187', '0.9231', '0.9321', '0.9346', '0.9302']\n",
            "  Cat A  : ['0.9134', '0.9254', '0.9360', '0.9306', '0.9308']\n",
            "  Cat B  : ['0.9117', '0.9217', '0.9342', '0.9305', '0.9264']\n",
            "\n",
            "â”€â”€ Wilcoxon Signed-Rank Tests (Pairwise AUC-ROC, n=5 folds) â”€â”€\n",
            "\n",
            "  CatBoost (Model A) vs. Logistic Regression\n",
            "  W = 0.0   p = 0.0625   Direction: A > B (+:5, -:0, ties:0)\n",
            "  [Note: min. achievable p with n=5 is â‰ˆ0.0625]\n",
            "  â†’ Borderline: consistent direction, limited by n=5 power constraint\n",
            "\n",
            "  XGBoost vs. Logistic Regression\n",
            "  W = 0.0   p = 0.0625   Direction: A > B (+:5, -:0, ties:0)\n",
            "  [Note: min. achievable p with n=5 is â‰ˆ0.0625]\n",
            "  â†’ Borderline: consistent direction, limited by n=5 power constraint\n",
            "\n",
            "  CatBoost (Model A) vs. XGBoost\n",
            "  W = 6.0   p = 0.8125   Direction: A > B (+:3, -:2, ties:0)\n",
            "  [Note: min. achievable p with n=5 is â‰ˆ0.0625]\n",
            "  â†’ FAIL TO REJECT null at Î±=0.05\n",
            "\n",
            "â”€â”€ DeLong's Test (Ablation: Model A vs. Model B) â”€â”€\n",
            "\n",
            "  Model A (Full) AUC (DeLong):  0.9269\n",
            "  Model B (Ablated) AUC (DeLong): 0.9246\n",
            "  Î”AUC = -0.0023\n",
            "  DeLong z-statistic = 2.5437\n",
            "  p-value = 0.0110\n",
            "  â†’ REJECT Hâ‚€â‚‚ at Î±=0.05: Macroeconomic exclusion significantly degrades performance âœ…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 11 â€” CONFUSION MATRIX (OUT-OF-FOLD AGGREGATION)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"â•\"*65)\n",
        "print(\"  CONFUSION MATRIX â€” CatBoost (Out-of-Fold Aggregation)\")\n",
        "print(\"â•\"*65)\n",
        "\n",
        "# Aggregate all OOF predictions (threshold = 0.50)\n",
        "# This sums to exactly n=4,424 instances\n",
        "oof_preds_A  = (all_probs_A >= 0.50).astype(int)\n",
        "cm = confusion_matrix(all_y_true_A, oof_preds_A)\n",
        "\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "total = TN + FP + FN + TP\n",
        "\n",
        "print(f\"\\n  Confusion Matrix (n = {total}):\")\n",
        "print(f\"  {'':30s} Predicted Non-Dropout  Predicted Dropout\")\n",
        "print(f\"  {'Actual Non-Dropout':30s}  TN = {TN:5d}          FP = {FP:5d}\")\n",
        "print(f\"  {'Actual Dropout':30s}  FN = {FN:5d}          TP = {TP:5d}\")\n",
        "print(f\"\\n  Total instances: {total}  (should equal {len(y)})\")\n",
        "\n",
        "# Class-specific metrics\n",
        "precision_dp = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall_dp    = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "fpr_dp       = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "fnr_dp       = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
        "accuracy_oof = (TP + TN) / total\n",
        "\n",
        "print(f\"\\n  Dropout class (positive) metrics:\")\n",
        "print(f\"    Precision    : {precision_dp:.4f} ({precision_dp*100:.1f}%)\")\n",
        "print(f\"    Recall       : {recall_dp:.4f} ({recall_dp*100:.1f}%)\")\n",
        "print(f\"    FPR          : {fpr_dp:.4f} ({fpr_dp*100:.1f}%)\")\n",
        "print(f\"    FNR          : {fnr_dp:.4f} ({fnr_dp*100:.1f}%)\")\n",
        "print(f\"    OOF Accuracy : {accuracy_oof:.4f} ({accuracy_oof*100:.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Rxhpl6sKuA",
        "outputId": "df01bda6-597d-4e1d-84ab-4239184d4a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "  CONFUSION MATRIX â€” CatBoost (Out-of-Fold Aggregation)\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "  Confusion Matrix (n = 4424):\n",
            "                                 Predicted Non-Dropout  Predicted Dropout\n",
            "  Actual Non-Dropout              TN =  2709          FP =   294\n",
            "  Actual Dropout                  FN =   274          TP =  1147\n",
            "\n",
            "  Total instances: 4424  (should equal 4424)\n",
            "\n",
            "  Dropout class (positive) metrics:\n",
            "    Precision    : 0.7960 (79.6%)\n",
            "    Recall       : 0.8072 (80.7%)\n",
            "    FPR          : 0.0979 (9.8%)\n",
            "    FNR          : 0.1928 (19.3%)\n",
            "    OOF Accuracy : 0.8716 (87.2%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 12 â€” SHAP ANALYSIS (CatBoost Model A, Final Fold)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"â•\"*65)\n",
        "print(\"  SHAP ANALYSIS â€” CatBoost (Model A, Final Fold Validation Set)\")\n",
        "print(\"â•\"*65)\n",
        "print(\"  Training final CatBoost model for SHAP analysis...\")\n",
        "\n",
        "# Re-train CatBoost on fold 4 (index 4) training data for SHAP\n",
        "fold_list = list(skf.split(X_full, y))\n",
        "train_idx_final, val_idx_final = fold_list[4]\n",
        "\n",
        "X_train_shap_fold = X_full[train_idx_final]\n",
        "y_train_shap_fold = y[train_idx_final]\n",
        "X_val_shap_fold   = X_full[val_idx_final]\n",
        "y_val_shap_fold   = y[val_idx_final]\n",
        "\n",
        "# Use best params from fold 4\n",
        "best_p_shap = cat_A_best_params_per_fold[4]\n",
        "\n",
        "# Re-apply MI selection on this fold\n",
        "X_train_shap_sel, X_val_shap_sel, sel_idx_shap, mi_shap = apply_mi_feature_selection(\n",
        "    X_train_shap_fold, y_train_shap_fold, X_val_shap_fold\n",
        ")\n",
        "selected_feature_names_shap = [feature_names_full[i] for i in sel_idx_shap]\n",
        "cat_idx_shap = [selected_feature_names_shap.index(c) for c in cat_cols_valid\n",
        "                if c in selected_feature_names_shap]\n",
        "\n",
        "catboost_shap = CatBoostClassifier(\n",
        "    **best_p_shap,\n",
        "    # cat_features=cat_idx_shap,\n",
        "    class_weights={0: 1.0, 1: class_weight_ratio},\n",
        "    random_seed=RANDOM_SEED,\n",
        "    verbose=0,\n",
        "    allow_writing_files=False\n",
        ")\n",
        "catboost_shap.fit(X_train_shap_sel, y_train_shap_fold)\n",
        "print(\"  âœ… CatBoost SHAP model trained.\")\n",
        "\n",
        "# â”€â”€ Compute SHAP values â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"  Computing SHAP values (this may take 1-2 minutes)...\")\n",
        "explainer = shap.TreeExplainer(catboost_shap)\n",
        "shap_values = explainer.shap_values(X_val_shap_sel)\n",
        "\n",
        "# For binary classification CatBoost, SHAP may return a list [class0, class1]\n",
        "if isinstance(shap_values, list):\n",
        "    shap_vals = shap_values[1]  # class 1 = Dropout\n",
        "else:\n",
        "    shap_vals = shap_values\n",
        "\n",
        "print(f\"  SHAP values shape: {shap_vals.shape}\")\n",
        "\n",
        "# â”€â”€ Top 15 features by mean absolute SHAP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n",
        "shap_df = pd.DataFrame({\n",
        "    'Feature': selected_feature_names_shap,\n",
        "    'Mean_Abs_SHAP': mean_abs_shap\n",
        "}).sort_values('Mean_Abs_SHAP', ascending=False).head(15)\n",
        "\n",
        "print(\"\\n  Top 15 Features by Mean |SHAP|:\")\n",
        "print(f\"  {'Rank':<6} {'Feature':<45} {'Mean |SHAP|':>12}\")\n",
        "print(f\"  {'â”€'*63}\")\n",
        "for rank, (_, row) in enumerate(shap_df.iterrows(), 1):\n",
        "    print(f\"  {rank:<6} {row['Feature']:<45} {row['Mean_Abs_SHAP']:>12.5f}\")\n",
        "\n",
        "# Save SHAP data\n",
        "shap_df.to_csv(f\"{OUTPUT_DIR}/shap_importance.csv\", index=False)\n",
        "print(f\"\\n  ğŸ’¾ SHAP importance saved to {OUTPUT_DIR}/shap_importance.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI9lJpeUsKwW",
        "outputId": "ef9ad444-3357-40c6-87dd-1fce207989fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "  SHAP ANALYSIS â€” CatBoost (Model A, Final Fold Validation Set)\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "  Training final CatBoost model for SHAP analysis...\n",
            "  âœ… CatBoost SHAP model trained.\n",
            "  Computing SHAP values (this may take 1-2 minutes)...\n",
            "  SHAP values shape: (884, 36)\n",
            "\n",
            "  Top 15 Features by Mean |SHAP|:\n",
            "  Rank   Feature                                        Mean |SHAP|\n",
            "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  1      Curricular units 2nd sem (approved)                1.19879\n",
            "  2      Tuition fees up to date                            0.53144\n",
            "  3      Curricular units 1st sem (approved)                0.28195\n",
            "  4      Course                                             0.27125\n",
            "  5      Age at enrollment                                  0.26464\n",
            "  6      Curricular units 2nd sem (grade)                   0.24187\n",
            "  7      Gender                                             0.13685\n",
            "  8      Scholarship holder                                 0.13191\n",
            "  9      Curricular units 2nd sem (enrolled)                0.12841\n",
            "  10     Unemployment rate                                  0.10733\n",
            "  11     Application mode                                   0.10518\n",
            "  12     Debtor                                             0.08758\n",
            "  13     Mother's occupation                                0.08163\n",
            "  14     Curricular units 1st sem (grade)                   0.07858\n",
            "  15     Curricular units 1st sem (enrolled)                0.07780\n",
            "\n",
            "  ğŸ’¾ SHAP importance saved to paper_outputs/shap_importance.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 13 â€” GENERATE ALL FIGURES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"â•\"*65)\n",
        "print(\"  GENERATING FIGURES\")\n",
        "print(\"â•\"*65)\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'font.family': 'serif',\n",
        "    'font.size': 11,\n",
        "    'axes.titlesize': 13,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 10,\n",
        "    'ytick.labelsize': 10,\n",
        "    'figure.dpi': 150,\n",
        "    'savefig.dpi': 300,\n",
        "    'savefig.bbox': 'tight'\n",
        "})\n",
        "\n",
        "# â”€â”€ FIGURE 1: ROC Curves (All 3 Models) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\n  Generating Fig 1: ROC Curves...\")\n",
        "\n",
        "fig1, ax1 = plt.subplots(figsize=(7, 6))\n",
        "\n",
        "models_roc = {\n",
        "    'Logistic Regression': (lr_fold_metrics, 'steelblue',   '--'),\n",
        "    'XGBoost':             (xgb_fold_metrics, 'darkorange', '-.' ),\n",
        "    'CatBoost (Proposed)': (cat_A_fold_metrics,'darkgreen', '-'  ),\n",
        "}\n",
        "\n",
        "for name, (fold_metrics, color, ls) in models_roc.items():\n",
        "    mean_auc = np.mean([m['auc_roc'] for m in fold_metrics])\n",
        "    std_auc  = np.std([m['auc_roc'] for m in fold_metrics])\n",
        "\n",
        "    # Compute mean ROC from OOF predictions\n",
        "    # We need fold-specific predictions â€” use stored probs where available\n",
        "    # For a clean ROC: use fold 4 predictions (representative fold)\n",
        "    if name == 'Logistic Regression':\n",
        "        fi = fold_list[4]\n",
        "        X_tr, X_vl = X_full[fi[0]], X_full[fi[1]]\n",
        "        y_tr, y_vl = y[fi[0]], y[fi[1]]\n",
        "        Xtr_sel, Xvl_sel, _, _ = apply_mi_feature_selection(X_tr, y_tr, X_vl)\n",
        "        scaler_roc = StandardScaler()\n",
        "        Xtr_sc = scaler_roc.fit_transform(Xtr_sel)\n",
        "        Xvl_sc = scaler_roc.transform(Xvl_sel)\n",
        "        best_C  = lr_best_params_per_fold[4]['C']\n",
        "        roc_model = LogisticRegression(C=best_C, class_weight='balanced',\n",
        "                                        max_iter=2000, random_state=RANDOM_SEED)\n",
        "        roc_model.fit(Xtr_sc, y_tr)\n",
        "        prob_roc = roc_model.predict_proba(Xvl_sc)[:, 1]\n",
        "        fpr_r, tpr_r, _ = roc_curve(y_vl, prob_roc)\n",
        "    elif name == 'XGBoost':\n",
        "        prob_roc = list(cat_A_val_probs.values())[4]  # placeholder â€” replace if stored\n",
        "        # Recompute for fold 4\n",
        "        fi = fold_list[4]\n",
        "        X_tr, X_vl = X_full[fi[0]], X_full[fi[1]]\n",
        "        y_tr, y_vl = y[fi[0]], y[fi[1]]\n",
        "        Xtr_sel, Xvl_sel, _, _ = apply_mi_feature_selection(X_tr, y_tr, X_vl)\n",
        "        bp = xgb_best_params_per_fold[4]\n",
        "        roc_model = xgb.XGBClassifier(**bp, scale_pos_weight=class_weight_ratio,\n",
        "                                       use_label_encoder=False, eval_metric='auc',\n",
        "                                       random_state=RANDOM_SEED, n_jobs=-1)\n",
        "        roc_model.fit(Xtr_sel, y_tr)\n",
        "        prob_roc = roc_model.predict_proba(Xvl_sel)[:, 1]\n",
        "        fpr_r, tpr_r, _ = roc_curve(y_vl, prob_roc)\n",
        "    else:  # CatBoost\n",
        "        prob_roc = cat_A_val_probs[4]\n",
        "        fpr_r, tpr_r, _ = roc_curve(cat_A_val_labels[4], prob_roc)\n",
        "\n",
        "    ax1.plot(fpr_r, tpr_r, color=color, linestyle=ls, lw=2,\n",
        "             label=f\"{name} (AUC = {mean_auc:.3f} Â± {std_auc:.3f})\")\n",
        "\n",
        "ax1.plot([0, 1], [0, 1], 'k:', lw=1.2, label='Random Classifier (AUC = 0.500)')\n",
        "ax1.fill_between([0, 1], [0, 1], alpha=0.04, color='gray')\n",
        "ax1.set_xlim([0.0, 1.0])\n",
        "ax1.set_ylim([0.0, 1.01])\n",
        "ax1.set_xlabel('False Positive Rate (1 âˆ’ Specificity)')\n",
        "ax1.set_ylabel('True Positive Rate (Sensitivity / Recall)')\n",
        "ax1.set_title('Fig. 1. ROC Curves â€” All Models (Fold 5 Validation Set)')\n",
        "ax1.legend(loc='lower right', fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.spines['top'].set_visible(False)\n",
        "ax1.spines['right'].set_visible(False)\n",
        "plt.tight_layout()\n",
        "fig1.savefig(f\"{OUTPUT_DIR}/fig1_roc_curves.png\")\n",
        "plt.show()\n",
        "print(f\"  ğŸ’¾ Saved: {OUTPUT_DIR}/fig1_roc_curves.png\")\n",
        "\n",
        "# â”€â”€ FIGURE 2: SHAP Summary Plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\n  Generating Fig 2: SHAP Summary Plot...\")\n",
        "\n",
        "fig2, ax2 = plt.subplots(figsize=(9, 7))\n",
        "shap_plot_vals = shap_vals\n",
        "shap_plot_data = X_val_shap_sel\n",
        "\n",
        "# Get top 15 feature indices\n",
        "top15_idx = shap_df.index.tolist()  # already sorted by importance\n",
        "top15_names = shap_df['Feature'].tolist()\n",
        "\n",
        "shap.summary_plot(\n",
        "    shap_plot_vals[:, :len(selected_feature_names_shap)],\n",
        "    shap_plot_data,\n",
        "    feature_names=selected_feature_names_shap,\n",
        "    max_display=15,\n",
        "    show=False,\n",
        "    plot_type=\"dot\",\n",
        "    color_bar_label=\"Feature Value\"\n",
        ")\n",
        "plt.title(\"Fig. 2. SHAP Summary Plot â€” CatBoost (Top 15 Features)\", fontsize=13, pad=12)\n",
        "plt.tight_layout()\n",
        "fig2.savefig(f\"{OUTPUT_DIR}/fig2_shap_summary.png\")\n",
        "plt.show()\n",
        "print(f\"  ğŸ’¾ Saved: {OUTPUT_DIR}/fig2_shap_summary.png\")\n",
        "\n",
        "# â”€â”€ FIGURE 3: SHAP Dependence Plot (Top Feature) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\n  Generating Fig 3: SHAP Dependence Plot...\")\n",
        "\n",
        "top1_feat_name = shap_df.iloc[0]['Feature']\n",
        "if top1_feat_name in selected_feature_names_shap:\n",
        "    top1_feat_idx = selected_feature_names_shap.index(top1_feat_name)\n",
        "    fig3, ax3 = plt.subplots(figsize=(8, 5))\n",
        "    shap.dependence_plot(\n",
        "        top1_feat_idx,\n",
        "        shap_plot_vals,\n",
        "        X_val_shap_sel,\n",
        "        feature_names=selected_feature_names_shap,\n",
        "        ax=ax3,\n",
        "        show=False\n",
        "    )\n",
        "    ax3.set_title(f\"Fig. 3. SHAP Dependence Plot â€” {top1_feat_name}\", fontsize=13)\n",
        "    ax3.set_xlabel(top1_feat_name)\n",
        "    ax3.set_ylabel(\"SHAP Value (Impact on Dropout Probability)\")\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.spines['top'].set_visible(False)\n",
        "    ax3.spines['right'].set_visible(False)\n",
        "    plt.tight_layout()\n",
        "    fig3.savefig(f\"{OUTPUT_DIR}/fig3_shap_dependence.png\")\n",
        "    plt.show()\n",
        "    print(f\"  ğŸ’¾ Saved: {OUTPUT_DIR}/fig3_shap_dependence.png\")\n",
        "\n",
        "# â”€â”€ FIGURE 4: Confusion Matrix Heatmap â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\n  Generating Fig 4: Confusion Matrix...\")\n",
        "\n",
        "fig4, ax4 = plt.subplots(figsize=(6, 5))\n",
        "cm_display = np.array([[TN, FP], [FN, TP]])\n",
        "labels = [['TN\\n{:,}'.format(TN), 'FP\\n{:,}'.format(FP)],\n",
        "          ['FN\\n{:,}'.format(FN), 'TP\\n{:,}'.format(TP)]]\n",
        "\n",
        "sns.heatmap(\n",
        "    cm_display,\n",
        "    annot=np.array(labels), fmt='',\n",
        "    cmap='Blues',\n",
        "    xticklabels=['Predicted\\nNon-Dropout', 'Predicted\\nDropout'],\n",
        "    yticklabels=['Actual\\nNon-Dropout', 'Actual\\nDropout'],\n",
        "    linewidths=1, linecolor='white',\n",
        "    cbar_kws={'label': 'Count'},\n",
        "    ax=ax4\n",
        ")\n",
        "ax4.set_title(\n",
        "    f\"Fig. 4. Confusion Matrix â€” CatBoost (OOF Aggregation, n={total:,})\\n\"\n",
        "    f\"Precision={precision_dp:.3f}  Recall={recall_dp:.3f}  Accuracy={accuracy_oof:.3f}\",\n",
        "    fontsize=11, pad=10\n",
        ")\n",
        "plt.tight_layout()\n",
        "fig4.savefig(f\"{OUTPUT_DIR}/fig4_confusion_matrix.png\")\n",
        "plt.show()\n",
        "print(f\"  ğŸ’¾ Saved: {OUTPUT_DIR}/fig4_confusion_matrix.png\")\n",
        "\n",
        "print(\"\\n  âœ… All figures saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5IAWsRHsKys",
        "outputId": "59524ed8-9714-4dea-f475-d9ebfeed17d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "  GENERATING FIGURES\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "  Generating Fig 1: ROC Curves...\n",
            "  ğŸ’¾ Saved: paper_outputs/fig1_roc_curves.png\n",
            "\n",
            "  Generating Fig 2: SHAP Summary Plot...\n",
            "  ğŸ’¾ Saved: paper_outputs/fig2_shap_summary.png\n",
            "\n",
            "  Generating Fig 3: SHAP Dependence Plot...\n",
            "  ğŸ’¾ Saved: paper_outputs/fig3_shap_dependence.png\n",
            "\n",
            "  Generating Fig 4: Confusion Matrix...\n",
            "  ğŸ’¾ Saved: paper_outputs/fig4_confusion_matrix.png\n",
            "\n",
            "  âœ… All figures saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 14 â€” HYPERPARAMETER SUMMARY TABLE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"â•\"*65)\n",
        "print(\"  FINAL HYPERPARAMETER CONFIGURATIONS\")\n",
        "print(\"â•\"*65)\n",
        "\n",
        "print(\"\\n  Logistic Regression â€” Best C per fold:\")\n",
        "for i, p in enumerate(lr_best_params_per_fold):\n",
        "    print(f\"    Fold {i+1}: C = {p['C']}\")\n",
        "\n",
        "print(\"\\n  XGBoost â€” Best params per fold:\")\n",
        "xgb_param_keys = ['n_estimators', 'max_depth', 'learning_rate', 'subsample',\n",
        "                   'colsample_bytree', 'reg_lambda', 'reg_alpha']\n",
        "for i, p in enumerate(xgb_best_params_per_fold):\n",
        "    print(f\"    Fold {i+1}: n_est={p.get('n_estimators')}  depth={p.get('max_depth')}  \"\n",
        "          f\"lr={p.get('learning_rate', 0):.4f}  sub={p.get('subsample', 0):.2f}\")\n",
        "\n",
        "print(\"\\n  CatBoost Model A â€” Best params per fold:\")\n",
        "for i, p in enumerate(cat_A_best_params_per_fold):\n",
        "    print(f\"    Fold {i+1}: iter={p.get('iterations')}  depth={p.get('depth')}  \"\n",
        "          f\"lr={p.get('learning_rate', 0):.4f}  l2={p.get('l2_leaf_reg', 0):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPu-O8d2r8N6",
        "outputId": "ebe48c88-2cf9-4e1f-aab4-ae065b6a2032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "  FINAL HYPERPARAMETER CONFIGURATIONS\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "  Logistic Regression â€” Best C per fold:\n",
            "    Fold 1: C = 0.1\n",
            "    Fold 2: C = 10\n",
            "    Fold 3: C = 0.1\n",
            "    Fold 4: C = 1\n",
            "    Fold 5: C = 0.1\n",
            "\n",
            "  XGBoost â€” Best params per fold:\n",
            "    Fold 1: n_est=951  depth=3  lr=0.0183  sub=0.94\n",
            "    Fold 2: n_est=389  depth=9  lr=0.0177  sub=0.97\n",
            "    Fold 3: n_est=399  depth=3  lr=0.0518  sub=0.94\n",
            "    Fold 4: n_est=740  depth=3  lr=0.0232  sub=0.71\n",
            "    Fold 5: n_est=506  depth=3  lr=0.0256  sub=0.88\n",
            "\n",
            "  CatBoost Model A â€” Best params per fold:\n",
            "    Fold 1: iter=763  depth=6  lr=0.0130  l2=0.3570\n",
            "    Fold 2: iter=979  depth=6  lr=0.0105  l2=9.6398\n",
            "    Fold 3: iter=865  depth=5  lr=0.0151  l2=2.0011\n",
            "    Fold 4: iter=903  depth=7  lr=0.0128  l2=6.0948\n",
            "    Fold 5: iter=628  depth=5  lr=0.0210  l2=3.3054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 15 â€” COMPLETE RESULTS EXPORT FOR PAPER\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"â•\"*65)\n",
        "print(\"  COMPLETE RESULTS â€” READY TO PASTE INTO PAPER TABLES\")\n",
        "print(\"â•\"*65)\n",
        "\n",
        "def fmt(mean, std):\n",
        "    return f\"{mean:.3f} Â± {std:.3f}\"\n",
        "\n",
        "metrics_order = ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']\n",
        "col_names     = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
        "\n",
        "print(\"\\n  â”Œâ”€ TABLE 2: Model Performance Comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
        "print(f\"  â”‚  {'Model':<35} {'Accuracy':>14} {'Precision':>14} {'Recall':>14} {'F1-Score':>14} {'AUC-ROC':>14} â”‚\")\n",
        "print(f\"  â”‚  {'â”€'*35} {'â”€'*14} {'â”€'*14} {'â”€'*14} {'â”€'*14} {'â”€'*14} â”‚\")\n",
        "\n",
        "for model_name, summary in [\n",
        "    (\"Logistic Regression (Baseline)\", lr_summary),\n",
        "    (\"XGBoost\",                        xgb_summary),\n",
        "    (\"CatBoost (Proposed)\",            cat_A_summary),\n",
        "]:\n",
        "    row = \"  â”‚  \" + f\"{model_name:<35}\"\n",
        "    for m in metrics_order:\n",
        "        row += f\" {fmt(summary[m]['mean'], summary[m]['std']):>14}\"\n",
        "    row += \" â”‚\"\n",
        "    print(row)\n",
        "print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
        "\n",
        "print(\"\\n  â”Œâ”€ TABLE 3: Confusion Matrix (CatBoost OOF, n=4,424) â”€â”\")\n",
        "print(f\"  â”‚  {'':30s} {'Pred: Non-Dropout':>20} {'Pred: Dropout':>20} {'Total':>10} â”‚\")\n",
        "print(f\"  â”‚  {'Actual: Non-Dropout':30s} {'TN = '+str(TN):>20} {'FP = '+str(FP):>20} {TN+FP:>10} â”‚\")\n",
        "print(f\"  â”‚  {'Actual: Dropout':30s} {'FN = '+str(FN):>20} {'TP = '+str(TP):>20} {FN+TP:>10} â”‚\")\n",
        "print(f\"  â”‚  {'Total':30s} {TN+FN:>20} {FP+TP:>20} {total:>10} â”‚\")\n",
        "print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
        "\n",
        "print(\"\\n  â”Œâ”€ TABLE 4: Ablation Study â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
        "print(f\"  â”‚  {'Configuration':<45} {'Accuracy':>14} {'F1-Score':>14} {'AUC-ROC':>14} {'Î”AUC':>8} â”‚\")\n",
        "print(f\"  â”‚  {'â”€'*45} {'â”€'*14} {'â”€'*14} {'â”€'*14} {'â”€'*8} â”‚\")\n",
        "\n",
        "delta_auc = cat_B_summary['auc_roc']['mean'] - cat_A_summary['auc_roc']['mean']\n",
        "for lbl, summ, d_str in [\n",
        "    (\"Model A: Full Feature Set (35 features)\",               cat_A_summary, \"â€”\"),\n",
        "    (\"Model B: Without Macroeconomic Ind. (32 features)\",     cat_B_summary, f\"{delta_auc:+.3f}\"),\n",
        "]:\n",
        "    row = f\"  â”‚  {lbl:<45}\"\n",
        "    for m in ['accuracy', 'f1', 'auc_roc']:\n",
        "        row += f\" {fmt(summ[m]['mean'], summ[m]['std']):>14}\"\n",
        "    row += f\" {d_str:>8} â”‚\"\n",
        "    print(row)\n",
        "print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
        "\n",
        "print(\"\\n  â”Œâ”€ TABLE 5: Statistical Significance Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
        "print(f\"  â”‚  {'Comparison':<45} {'Test':>20} {'Statistic':>14} {'p-value':>10} â”‚\")\n",
        "print(f\"  â”‚  {'â”€'*45} {'â”€'*20} {'â”€'*14} {'â”€'*10} â”‚\")\n",
        "\n",
        "wilcoxon_results = [\n",
        "    (\"CatBoost vs. LR (AUC)\",       \"Wilcoxon S-R\",  w1,        p1),\n",
        "    (\"XGBoost vs. LR (AUC)\",        \"Wilcoxon S-R\",  w2,        p2),\n",
        "    (\"CatBoost vs. XGBoost (AUC)\",  \"Wilcoxon S-R\",  w3,        p3),\n",
        "    (\"Model A vs. B (DeLong's)\",    \"DeLong's Test\", z_delong,  p_delong),\n",
        "]\n",
        "for comp, test, stat, pval in wilcoxon_results:\n",
        "    stat_str = f\"{stat:.4f}\" if stat is not None else \"N/A\"\n",
        "    pval_str = f\"{pval:.4f}\" if pval is not None else \"N/A\"\n",
        "    print(f\"  â”‚  {comp:<45} {test:>20} {stat_str:>14} {pval_str:>10} â”‚\")\n",
        "print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
        "\n",
        "# â”€â”€ Save all numerical results to JSON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "results_export = {\n",
        "    \"logistic_regression\": {m: {\"mean\": lr_summary[m]['mean'], \"std\": lr_summary[m]['std'],\n",
        "                                 \"fold_values\": lr_summary[m]['values']} for m in metrics_order},\n",
        "    \"xgboost\":             {m: {\"mean\": xgb_summary[m]['mean'], \"std\": xgb_summary[m]['std'],\n",
        "                                 \"fold_values\": xgb_summary[m]['values']} for m in metrics_order},\n",
        "    \"catboost_model_A\":    {m: {\"mean\": cat_A_summary[m]['mean'], \"std\": cat_A_summary[m]['std'],\n",
        "                                 \"fold_values\": cat_A_summary[m]['values']} for m in metrics_order},\n",
        "    \"catboost_model_B\":    {m: {\"mean\": cat_B_summary[m]['mean'], \"std\": cat_B_summary[m]['std'],\n",
        "                                 \"fold_values\": cat_B_summary[m]['values']} for m in metrics_order},\n",
        "    \"confusion_matrix\":    {\"TN\": int(TN), \"FP\": int(FP), \"FN\": int(FN), \"TP\": int(TP),\n",
        "                             \"total\": int(total), \"precision\": precision_dp,\n",
        "                             \"recall\": recall_dp, \"fpr\": fpr_dp, \"accuracy_oof\": accuracy_oof},\n",
        "    \"statistical_tests\": {\n",
        "        \"wilcoxon_catboost_vs_lr\":   {\"W\": w1, \"p\": p1},\n",
        "        \"wilcoxon_xgboost_vs_lr\":    {\"W\": w2, \"p\": p2},\n",
        "        \"wilcoxon_catboost_vs_xgb\":  {\"W\": w3, \"p\": p3},\n",
        "        \"delong_modelA_vs_modelB\":   {\"z\": z_delong, \"p\": p_delong,\n",
        "                                       \"auc_A\": auc_A_full, \"auc_B\": auc_B_full,\n",
        "                                       \"delta_auc\": float(auc_B_full - auc_A_full)},\n",
        "    },\n",
        "    \"shap_top15\": shap_df.to_dict(orient='records'),\n",
        "    \"hyperparameters\": {\n",
        "        \"lr_best_C_per_fold\":       lr_best_params_per_fold,\n",
        "        \"xgb_best_params_per_fold\": xgb_best_params_per_fold,\n",
        "        \"catboost_A_best_per_fold\": cat_A_best_params_per_fold,\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/all_results.json\", 'w') as f:\n",
        "    json.dump(results_export, f, indent=2)\n",
        "\n",
        "# Save summary CSVs\n",
        "summary_rows = []\n",
        "for model_name, summary in [\n",
        "    (\"Logistic Regression\", lr_summary),\n",
        "    (\"XGBoost\",             xgb_summary),\n",
        "    (\"CatBoost Model A\",    cat_A_summary),\n",
        "    (\"CatBoost Model B\",    cat_B_summary),\n",
        "]:\n",
        "    row = {\"Model\": model_name}\n",
        "    for m in metrics_order:\n",
        "        row[f\"{m}_mean\"] = round(summary[m]['mean'], 5)\n",
        "        row[f\"{m}_std\"]  = round(summary[m]['std'],  5)\n",
        "    summary_rows.append(row)\n",
        "pd.DataFrame(summary_rows).to_csv(f\"{OUTPUT_DIR}/model_performance.csv\", index=False)\n",
        "\n",
        "print(f\"\\n  ğŸ’¾ All results saved:\")\n",
        "print(f\"     {OUTPUT_DIR}/all_results.json        â† Complete numerical results\")\n",
        "print(f\"     {OUTPUT_DIR}/model_performance.csv   â† Table 2 data\")\n",
        "print(f\"     {OUTPUT_DIR}/shap_importance.csv     â† SHAP rankings\")\n",
        "print(f\"     {OUTPUT_DIR}/fig1_roc_curves.png     â† Fig 1\")\n",
        "print(f\"     {OUTPUT_DIR}/fig2_shap_summary.png   â† Fig 2\")\n",
        "print(f\"     {OUTPUT_DIR}/fig3_shap_dependence.pngâ† Fig 3\")\n",
        "print(f\"     {OUTPUT_DIR}/fig4_confusion_matrix.pngâ† Fig 4\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V72vVTw0sqpO",
        "outputId": "fe95b206-fabf-4a71-869e-25d726baff79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "  COMPLETE RESULTS â€” READY TO PASTE INTO PAPER TABLES\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n",
            "  â”Œâ”€ TABLE 2: Model Performance Comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "  â”‚  Model                                     Accuracy      Precision         Recall       F1-Score        AUC-ROC â”‚\n",
            "  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n",
            "  â”‚  Logistic Regression (Baseline)       0.866 Â± 0.006  0.776 Â± 0.015  0.820 Â± 0.004  0.797 Â± 0.007  0.918 Â± 0.006 â”‚\n",
            "  â”‚  XGBoost                              0.871 Â± 0.007  0.798 Â± 0.015  0.802 Â± 0.018  0.800 Â± 0.011  0.928 Â± 0.006 â”‚\n",
            "  â”‚  CatBoost (Proposed)                  0.872 Â± 0.007  0.796 Â± 0.012  0.807 Â± 0.010  0.802 Â± 0.011  0.927 Â± 0.008 â”‚\n",
            "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\n",
            "  â”Œâ”€ TABLE 3: Confusion Matrix (CatBoost OOF, n=4,424) â”€â”\n",
            "  â”‚                                    Pred: Non-Dropout        Pred: Dropout      Total â”‚\n",
            "  â”‚  Actual: Non-Dropout                       TN = 2709             FP = 294       3003 â”‚\n",
            "  â”‚  Actual: Dropout                            FN = 274            TP = 1147       1421 â”‚\n",
            "  â”‚  Total                                          2983                 1441       4424 â”‚\n",
            "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\n",
            "  â”Œâ”€ TABLE 4: Ablation Study â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "  â”‚  Configuration                                       Accuracy       F1-Score        AUC-ROC     Î”AUC â”‚\n",
            "  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n",
            "  â”‚  Model A: Full Feature Set (35 features)        0.872 Â± 0.007  0.802 Â± 0.011  0.927 Â± 0.008        â€” â”‚\n",
            "  â”‚  Model B: Without Macroeconomic Ind. (32 features)  0.868 Â± 0.007  0.796 Â± 0.011  0.925 Â± 0.008   -0.002 â”‚\n",
            "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\n",
            "  â”Œâ”€ TABLE 5: Statistical Significance Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "  â”‚  Comparison                                                    Test      Statistic    p-value â”‚\n",
            "  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n",
            "  â”‚  CatBoost vs. LR (AUC)                                 Wilcoxon S-R         0.0000     0.0625 â”‚\n",
            "  â”‚  XGBoost vs. LR (AUC)                                  Wilcoxon S-R         0.0000     0.0625 â”‚\n",
            "  â”‚  CatBoost vs. XGBoost (AUC)                            Wilcoxon S-R         6.0000     0.8125 â”‚\n",
            "  â”‚  Model A vs. B (DeLong's)                             DeLong's Test         2.5437     0.0110 â”‚\n",
            "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "\n",
            "  ğŸ’¾ All results saved:\n",
            "     paper_outputs/all_results.json        â† Complete numerical results\n",
            "     paper_outputs/model_performance.csv   â† Table 2 data\n",
            "     paper_outputs/shap_importance.csv     â† SHAP rankings\n",
            "     paper_outputs/fig1_roc_curves.png     â† Fig 1\n",
            "     paper_outputs/fig2_shap_summary.png   â† Fig 2\n",
            "     paper_outputs/fig3_shap_dependence.pngâ† Fig 3\n",
            "     paper_outputs/fig4_confusion_matrix.pngâ† Fig 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CELL 16 â€” DOWNLOAD ALL OUTPUTS AS ZIP\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import shutil\n",
        "\n",
        "zip_name = \"paper_outputs\"\n",
        "shutil.make_archive(zip_name, 'zip', OUTPUT_DIR)\n",
        "files.download(f\"{zip_name}.zip\")\n",
        "\n",
        "print(f\"\\nâœ… PIPELINE COMPLETE.\")\n",
        "print(f\"ğŸ“¦ Download started: {zip_name}.zip\")\n",
        "print(f\"\\nğŸ“‹ NEXT STEPS:\")\n",
        "print(f\"   1. Open paper_outputs/all_results.json\")\n",
        "print(f\"   2. Replace all placeholder numbers in your paper with the real values\")\n",
        "print(f\"   3. Insert the 4 saved figures into your Word document\")\n",
        "print(f\"   4. Generate Figures 5 and 6 (System Architecture, Preprocessing Flowchart)\")\n",
        "print(f\"      using draw.io, Lucidchart, or PowerPoint â€” these cannot be auto-generated\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "Vtqmlmx0srg9",
        "outputId": "596a4423-093e-4032-fdc8-5bfd860a894e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_16f74691-5e42-47b3-8a7e-647a6fe670f6\", \"paper_outputs.zip\", 886717)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… PIPELINE COMPLETE.\n",
            "ğŸ“¦ Download started: paper_outputs.zip\n",
            "\n",
            "ğŸ“‹ NEXT STEPS:\n",
            "   1. Open paper_outputs/all_results.json\n",
            "   2. Replace all placeholder numbers in your paper with the real values\n",
            "   3. Insert the 4 saved figures into your Word document\n",
            "   4. Generate Figures 5 and 6 (System Architecture, Preprocessing Flowchart)\n",
            "      using draw.io, Lucidchart, or PowerPoint â€” these cannot be auto-generated\n"
          ]
        }
      ]
    }
  ]
}